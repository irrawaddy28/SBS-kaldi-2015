#!/bin/bash -e

# Copyright 2015-2016  University of Illinois (Author: Amit Das)
# Apache 2.0
#

# Top most level script to train and test GMM-HMM and DNN for speech 
# recognition using probablistic transcripts (PT) generated 
# from crowdsource workers
#
# Try:
# 1) looping with increasing DT pseudo labels ... run for each language.
#  

echo "$0 $@"  # Print the command line for logging

[ -f path.sh ] && . ./path.sh;
[ -f cmd.sh ] && . ./cmd.sh;

## Options
stage=0

# PT-Dir
dir_raw_pt=$SBS_DATADIR/pt-lats

# DNN configurations
cmvn_opts=" --norm-means=true --norm-vars=true "
delta_order=0   # Use 1 for delta, 2 for delta-delta
splice=5
splice_step=1
min_iters=3

# Teacher-Student training options
# teacher_student=false       # false|true, if teacher_student=true, then enable T-S training
# posterior_temperature=-1  # Apply softmax with temperature to posteriors in PT/semisup. This is done only when temp > 0

# end config

. utils/parse_options.sh || exit 1;


# Usage:  nohup ./run.sh "AM AR CA DI HG MD" "SW" > run.log 2>&1 &
TRAIN_LANG=$1  # Example: "AM AR CA HG MD SW"
TEST_LANG=$2   # Example: "DI"
UNILANG_CODE=$(echo $TRAIN_LANG |sed 's/ /_/g')

dir_raw_pt=$dir_raw_pt/${TEST_LANG}
dbn_dir=exp/dnn4_pretrain-dbn/${TEST_LANG}
dnn_dir=exp/dnn4_pretrain-dbn_dnn/${TEST_LANG}
hmm_dir=exp/tri3c/${TEST_LANG}
ali_mono_dt_dir=exp/mono_ali/${TEST_LANG}
ali_dt_dir=exp/tri3c_ali/${TEST_LANG}
ali_pt_dir=exp/tri3cpt_ali/${TEST_LANG}
lats_pt_dir=${ali_pt_dir}/decode_train
data_fmllr_dir=data-fmllr-tri3c/${TEST_LANG}
data_bn_dir=data-bn/${TEST_LANG}
data_fmllr_bn_dir=data-fmllr-tri3c-bn/${TEST_LANG}

# =========================================
## Train a multilingual GMM-HMM system (exp/tri3b) using multilingual DT (deterministic transcripts) of training languages
## Test on a test language unseen during training
if [[ $stage -le 0 ]]; then
  ./run_hmm_multilingual.sh "${TRAIN_LANG}" "${TEST_LANG}"
fi
# =========================================

# =========================================
if [[ $stage -le 1 ]]; then
## Do a MAP adaptation of the multilingual GMM-HMM to PT (probabilistic transcripts) of test language. MAP adapted GMM-HMM is saved in exp/tri3c.
  ./run-pt-text-G-map-2.sh  "${TRAIN_LANG}" "${TEST_LANG}" ${dir_raw_pt}
fi
# =========================================

# =========================================
# Train and test a monolingual GMM-HMM (exp/monolingual/tri3b)
# using monolingual DT (deterministic transcripts)
if [[ $stage -le 2 ]]; then
  ./run_hmm_monolingual.sh --stage 1 "${TEST_LANG}"
fi
# =========================================

# =========================================
# Train and test a monolingual DNN system (exp/monolingual/dnn4_pretrain-dbn_dnn)
# using monolingual DT (deterministic transcripts)
if [[ $stage -le 3 ]]; then
  ./run_dnn_monolingual.sh ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
    "${TEST_LANG}" \
    exp/monolingual/tri3b/${TEST_LANG}  \
    exp/monolingual/tri3b_ali/${TEST_LANG} \
    exp/monolingual/data-fmllr-tri3b/${TEST_LANG} \
    exp/monolingual/dnn4_pretrain-dbn/${TEST_LANG}/indbn \
    exp/monolingual/dnn4_pretrain-dbn_dnn/${TEST_LANG}/monosoftmax_dt
fi
# =========================================

# =========================================
if [[ $stage -le 4 ]]; then
## Train a multilingual DNN system using multilingual DT of training languages. Use tri3c_ali as targets.
## This nnet, trained using DT, is used to provide a good initialization of the shared hidden layers (SHLs) using DBN pre-training. If we start training
## with both PT + DT, the SHLs may be unreliable.
  ./run_dnn_adapt_to_multi_dt.sh --stage 1 --train-dbn true \
    ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
    "${TRAIN_LANG}" "${TEST_LANG}" \
    ${hmm_dir} ${ali_dt_dir} ${data_fmllr_dir} \
    ${dbn_dir}/monosoftmax_dt ${dnn_dir}/monosoftmax_dt || exit 1;
fi
# =========================================

# =========================================
## Demonstrate the efficacy of a monosoftmax DNN trained with PTs where PTs are generated by an ASR system rather than crowdsource workers.
if [[ $stage -le 20 && ! $skip_selftrain ]]; then
  unsup_dir_tag="train"
  acwt=0.2
  feat_unsup_dir=data-fmllr-tri3b/${TEST_LANG}/${unsup_dir_tag}
  decoding_mdl_dir=${dnn_dir}/monosoftmax_dt # dnn mdl directory used to decode the unsup data
  lats_unsup_dir=${decoding_mdl_dir}/decode_${unsup_dir_tag}_text_G_${TEST_LANG} # dir where lattices generated by decoding unsup data will be saved
  
  # Now decode the training data using a reasonably well trained DNN model. The fMLLR transforms for train set are saved in tri3b_ali/decode_train
  # and the decoding lattices in the same DNN directory which is used for decoding the training data.
  # Note: We could also use GMM-HMM model tri3b/final.mdl for decoding but as of now the unsup lats scipt supports decoding using a nnet model.
  local/get_unsup_lats.sh --stage -2 --feats-nj 10 --unsup-dir-name ${unsup_dir_tag} \
    ${TEST_LANG} \
    exp/tri3b_ali/${TEST_LANG} \
    data-fmllr-tri3b/${TEST_LANG}/${TEST_LANG}/${unsup_dir_tag} \
    exp/tri3b/${TEST_LANG}/graph_text_G_${TEST_LANG}  \
    ${decoding_mdl_dir}  \
    ${lats_unsup_dir} || exit 1;
   
  # Copy the fMLLR transforms for dev and eval sets from tri3b to tri3b_ali
  rm -rf exp/tri3b_ali/${TEST_LANG}/decode_dev_${TEST_LANG} exp/tri3b_ali/${TEST_LANG}/decode_eval_${TEST_LANG}
  cp -Lr exp/tri3b/${TEST_LANG}/decode_dev_${TEST_LANG} exp/tri3b_ali/${TEST_LANG}/decode_dev_${TEST_LANG}
  cp -Lr exp/tri3b/${TEST_LANG}/decode_eval_${TEST_LANG} exp/tri3b_ali/${TEST_LANG}/decode_eval_${TEST_LANG}
  
  # Now fine tune the DNN using the decoded unsup lattice. Use different levels of frame weighting derived from best path lattice.
  # Use the fMLLR transforms from tri3b_ali/decode_* and training lattices from the DNN directory ${lats_unsup_dir}
  thresh=0.5
  ./run_dnn_adapt_to_mono_pt_frame_wt.sh --stage 1 --replace-softmax "true"  \
       --transform-dir-train "exp/tri3b_ali/${TEST_LANG}/decode_${unsup_dir_tag}_${TEST_LANG}" --threshold ${thresh} \
       ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
      "${TEST_LANG}" \
      exp/tri3b_ali/${TEST_LANG}  \
      ${lats_unsup_dir} \
      "${dnn_dir}/monosoftmax_dt/final.nnet" \
      data-fmllr-tri3b/${TEST_LANG} \
      ${dnn_dir}/monosoftmax_asrpt_fw${thresh} || exit 1;

  i=1
  for thresh in 0.6 0.7 0.8 0.9 ; do
    (./run_dnn_adapt_to_mono_pt_frame_wt.sh --stage 2 --replace-softmax "true" \
      --transform-dir-train "exp/tri3b_ali/${TEST_LANG}/decode_${unsup_dir_tag}_${TEST_LANG}"  --threshold ${thresh} \
      ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
      ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
      "${TEST_LANG}" \
      exp/tri3b_ali/${TEST_LANG}  \
      ${lats_unsup_dir} \
      "${dnn_dir}/monosoftmax_dt/final.nnet" \
      data-fmllr-tri3b/${TEST_LANG} \
      ${dnn_dir}/monosoftmax_asrpt_fw${thresh} || exit 1;) &
    i=$((i%N_BG)); ((i++==0)) && wait
  done
fi
# =========================================

# =========================================
## Single softmax DNN trained with crowd PTs
if [[ $stage -le 30 ]]; then
# Now, on top of the hidden layers of the multilingual DT system, create a new soft-max layer. This becomes a new DNN.
# Fine tune all layers of this new DNN using PT of the test language.
  ./run_dnn_adapt_to_mono_pt.sh --stage 1  --train-dbn false  --precomp-dnn "${dnn_dir}/monosoftmax_dt/final.nnet" --replace-softmax "true" \
    ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
    ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
    "${TRAIN_LANG}" \
    "${TEST_LANG}" \
    ${hmm_dir} \
    ${ali_pt_dir} \
    ${data_fmllr_dir} \
    ${dnn_dir}/monosoftmax_pt ${dnn_dir}/monosoftmax_pt || exit 1;
fi

if [[ $stage -le 31 ]]; then
# Now try the same thing using different levels of frame weighting derived from best path PT lattice. Do we get good improvements using frame weighting?
i=1
for thresh in 0.5 0.6 0.7 0.8 0.9 ; do
  nnet_outdir=${dnn_dir}/monosoftmax_pt_fw${thresh}
  (./run_dnn_adapt_to_mono_pt_frame_wt.sh --stage 2 --replace-softmax "true" \
    ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
    --threshold ${thresh} \
    ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
    "${TEST_LANG}" \
    ${ali_pt_dir} \
    ${lats_pt_dir} \
    "${dnn_dir}/monosoftmax_dt/final.nnet" \
    ${data_fmllr_dir} \
    ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1;) &
  i=$((i%N_BG)); ((i++==0)) && wait
done
fi
# =========================================

# =========================================
## MTL trained with crowd PT senones and DT senones; Task 1: CE, Task 2: CE
if [[ $stage -le 40 ]]; then
i=1
for thresh in 0.6; do # 0 0.2 0.4 0.6 0.8 (Only frames with frame weights above this threshold will be trained. Otherwise, ignored. From expts, the best setting is 0.6)
  for num_copies_2 in 0; do # 0 2 4 6
    for num_copies_1 in 2; do # 2 4
      for alpha in 1.0; do  # 0.2 0.4 0.6 0.8 1.0 (Weight for the secondary task. From expts, the best setting is 1.0 for all languages except SW. But SW PER is worse by only 0.1% for other alpha values)
        etag=type"ss"_fw${thresh}_cop${num_copies_1}${num_copies_2}_alpha${alpha}
        nnet_outdir=${dnn_dir}/multisoftmax_pt_$etag
        mkdir -p $nnet_outdir
        [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
        (./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
          --objective-csl "xent:xent" --lang-weight-csl "1.0:${alpha}"  --data-type-csl "pt:dt"  --label-type-csl "s:s" \
          --renew-nnet-type "blocksoftmax" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
          --threshold-csl "${thresh}:0.0" \
          --lat-dir-csl "${lats_pt_dir}:-" \
          --dup-and-merge-csl "${num_copies_1}>>1:${num_copies_2}>>2" \
          ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
          --min-iters ${min_iters} \
          ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
          "${TEST_LANG}:${UNILANG_CODE}" "${ali_pt_dir}:${ali_dt_dir}" \
          "${data_fmllr_dir}/${TEST_LANG}/train:${data_fmllr_dir}/${UNILANG_CODE}/train" ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
          ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1; ) &
        i=$((i%N_BG)); ((i++==0)) && wait
      done # alpha
    done  # num_copies_1
  done  # num_copies_2
done # thresh
fi
# =========================================

# =========================================
## MTL trained with crowd PT senones and DT senones; Task 1:  Target Interpolation (TI), Task 2: CE
if [[ $stage -le 42 ]]; then
i=1
for thresh in 0.6; do # 0.5 0.6 0.7 0.8 0.9 (Only frames with frame weights above this threshold will be trained. Otherwise, ignored)
  for num_copies_2 in 0; do # 0 2 4 6
    for num_copies_1 in 2; do # 2 4
      for alpha in 1.0; do  # 0.2 0.4 0.6 0.8 1.0
        # "hard": new tgt = rho*ground truth + (1-rho)*(1-hot posterior of nnet)
        # "soft": new tgt = rho*ground truth + (1-rho)*(posterior of nnet)
        # Note: When rho=1.0, there is no target interpolation. Hence, the training collapses to standard CE training.
        for tgt_interp_mode in "hard" "soft"; do
          for rho in 1 0.8 0.6 0.4 0.2; do
            etag=type"ss"_fw${thresh}_cop${num_copies_1}${num_copies_2}_alpha${alpha}_${tgt_interp_mode}_rho${rho}
            nnet_outdir=${dnn_dir}/multisoftmax_pt_$etag
            mkdir -p $nnet_outdir
            [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
            (./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
              --objective-csl "xent:xent" --lang-weight-csl "1.0:${alpha}"  --data-type-csl "pt:dt"  --label-type-csl "s:s" \
              --tgt-interp-mode "$tgt_interp_mode" --rho "$rho" \
              --renew-nnet-type "blocksoftmax" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
              --threshold-csl "${thresh}:0.0" \
              --lat-dir-csl "${lats_pt_dir}:-" \
              --dup-and-merge-csl "${num_copies_1}>>1:${num_copies_2}>>2" \
              ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
              --min-iters ${min_iters} \
              ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
              "${TEST_LANG}:${UNILANG_CODE}" "${ali_pt_dir}:${ali_dt_dir}" \
              "${data_fmllr_dir}/${TEST_LANG}/train:${data_fmllr_dir}/${UNILANG_CODE}/train" ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
              ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1; ) &
            i=$((i%N_BG)); ((i++==0)) && wait
          done # rho
        done # tgt_interp_mode
      done # alpha
    done  # num_copies_1
  done  # num_copies_2
done # thresh
fi
# =========================================


# =========================================
## MTL trained with crowd PT senones and DT senones; Task 1:  Knowledge Distillation (KD), Task 2: CE
if [[ $stage -le 44 ]]; then
i=1
for thresh in 0.0; do # 0.5 0.6 0.7 0.8 0.9 (Only frames with frame weights above this threshold will be trained. Otherwise, ignored)
  for num_copies_2 in 0; do # 0 2 4 6
    for num_copies_1 in 2; do # 2 4
      for alpha in 1.0; do  # 0.2 0.4 0.6 0.8 1.0
        for posterior_temperature in 2 3 4; do # modify posterior of PTs by applying a temperature softmax on the PTs
        # Note: When posterior_temperature = 0 AND rho_ts = 1, T/S training collapses to standard CE training
          for softmax_temperature in 2 3; do
            for rho_ts in  0.8 0.6 0.4 0.2; do # If rho_ts = 1, there is no effect of varying softmax_temperature.
              etag=type"ss"_fw${thresh}_cop${num_copies_1}${num_copies_2}_alpha${alpha}_Tpt${posterior_temperature}_T${softmax_temperature}_rho${rho_ts}
              nnet_outdir=${dnn_dir}/multisoftmax_pt_$etag
              mkdir -p $nnet_outdir
              [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
              (./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
                --objective-csl "ts:xent" --lang-weight-csl "1.0:${alpha}"  --data-type-csl "pt:dt"  --label-type-csl "s:s"   \
                --teacher-student "true"  --softmax-temperature $softmax_temperature  --rho-ts $rho_ts --mlp-teacher "${dnn_dir}/monosoftmax_dt/final.nnet" --posterior-temperature $posterior_temperature \
                --renew-nnet-type "blocksoftmax" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
                --threshold-csl "${thresh}:0.0" \
                --lat-dir-csl "${lats_pt_dir}:-" \
                --dup-and-merge-csl "${num_copies_1}>>1:${num_copies_2}>>2" \
                ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
                --min-iters ${min_iters} \
                ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
                "${TEST_LANG}:${UNILANG_CODE}" "${ali_pt_dir}:${ali_dt_dir}" \
                "${data_fmllr_dir}/${TEST_LANG}/train:${data_fmllr_dir}/${UNILANG_CODE}/train" ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
                ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1; ) &
              i=$((i%N_BG)); ((i++==0)) && wait
            done # rho_ts
          done # softmax_temperature
        done # posterior_temperature
      done # alpha      
    done  # num_copies_1
  done  # num_copies_2
done # thresh
fi
# =========================================
exit 0

# =========================================
## MTL trained with crowd PT senones and DT phones; Task 1: CE, Task 2: CE
if [[ $stage -le 44 ]]; then
i=1
for thresh in 0.6; do # 0.5 0.6 0.7 0.8 0.9
  for num_copies_2 in 0; do # 0 2 4 6
    for num_copies_1 in 2 4; do # 0 1 2 3 4
      for alpha_2 in 1.4 1.6 1.8; do
        for alpha_1 in 1.0 2.0; do
          etag=type"sp"_fw${thresh}_cop${num_copies_1}${num_copies_2}_alpha${alpha_1}${alpha_2}
          nnet_outdir=${dnn_dir}/multisoftmax_pt_$etag
          mkdir -p $nnet_outdir
          [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
          (./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
            --objective-csl "xent:xent" --lang-weight-csl "${alpha_1}:${alpha_2}" --data-type-csl "pt:dt"  --label-type-csl "s:p" \
            --renew-nnet-type "blocksoftmax" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
            --threshold-csl "${thresh}:0.0" \
            --lat-dir-csl "${lats_pt_dir}:-" \
            --dup-and-merge-csl "${num_copies_1}>>1:${num_copies_2}>>2" \
            ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
            --min-iters ${min_iters} \
            ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
            "${TEST_LANG}:${UNILANG_CODE}" "${ali_pt_dir}:${ali_mono_dt_dir}" \
            "${data_fmllr_dir}/${TEST_LANG}/train:${data_fmllr_dir}/${UNILANG_CODE}/train" ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
            ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1; ) &
          i=$((i%N_BG)); ((i++==0)) && wait
        done # alpha_1
      done # alpha_2
    done  # num_copies_1
  done  # num_copies_2
done # thresh
fi
# =========================================


# =========================================
## MTL trained with crowd PT senones, DT senones, DT phones; Task 1: CE, Task 2: CE, Task 3: CE
if [[ $stage -le 50 ]] && false; then
i=1
for thresh in 0.6; do # full range: 0.5 0.6 0.7 0.8 0.9
  for num_copies_3 in 0 2; do # full range: 0 2 4 6
    for num_copies_2 in 0; do # full range: 0 2 4 6
      for num_copies_1 in 2 4; do # full range: 0 1 2 3 4
        for alpha_3 in 1.4 1.6 1.8; do
          for alpha_2 in 0.2; do
            for alpha_1 in 1.0 2.0; do
              etag=type"ssp"_fw${thresh}_cop${num_copies_1}${num_copies_2}${num_copies_3}_alpha${alpha_1}${alpha_2}${alpha_3}
              nnet_outdir=${dnn_dir}/multisoftmax_pt_$etag
              mkdir -p $nnet_outdir
              [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
              (./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
                --objective-csl "xent:xent:xent" --lang-weight-csl "${alpha_1}:${alpha_2}:${alpha_3}" --data-type-csl "pt:dt:dt"  --label-type-csl "s:s:p" \
                --renew-nnet-type "blocksoftmax" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
                --threshold-csl "${thresh}:0.0:0.0" \
                --lat-dir-csl "${lats_pt_dir}:-:-" \
                --dup-and-merge-csl "${num_copies_1}>>1:${num_copies_2}>>2:${num_copies_3}>>3" \
                ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
                --min-iters ${min_iters} \
                ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
                "${TEST_LANG}:${UNILANG_CODE}:${UNILANG_CODE}" "${ali_pt_dir}:${ali_dt_dir}:${ali_mono_dt_dir}" \
                "${data_fmllr_dir}/${TEST_LANG}/train:${data_fmllr_dir}/${UNILANG_CODE}/train:${data_fmllr_dir}/${UNILANG_CODE}/train" ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
                ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1; ) &
              i=$((i%N_BG)); ((i++==0)) && wait
            done # alpha 1
          done # alpha 2 
        done # alpha 3
      done # num_copies_1
    done # num_copies_2
  done  # num_copies_3
done # thresh
fi
# =========================================

# =========================================
## Train MTL with crowd PT senones: DT senones: Autoencoder; Task 1: CE, Task 2: CE, Task 3: MSE
nutts=6000
unsup_dir_name="unsup_$nutts"
feat_unsup_dir=${data_fmllr_dir}/${TEST_LANG}/${unsup_dir_name}
if [[ $stage -le 60 ]]; then

  # Extract the fMLLR features of unsup data using the PT adapted HMM model
  if [ ! -f ${ali_pt_dir}/decode_${unsup_dir_name}_${TEST_LANG}/lat.1.gz ]; then
    local/get_unsup_lats.sh --nutts ${nutts} --unsup-dir-name ${unsup_dir_name} --skip-decode true \
      ${TEST_LANG} \
      ${ali_pt_dir} \
      ${feat_unsup_dir} \
      ${hmm_dir}/graph_text_G_${TEST_LANG}  \
      dummy  \
      dummy || exit 1;
  fi
  
  feat_pt_dir=${data_fmllr_dir}/${TEST_LANG}/train
  feat_dt_dir=${data_fmllr_dir}/${UNILANG_CODE}/train
  
  i=1
  for nutts_subset in 4000; do  # 4000 3000 2000 1000
    for thresh_pt in 0.6 ; do  # 0.6 0.7 0.8
      for num_copies_3 in 0; do # 0 1
        for num_copies_2 in 0 2; do # 0 2 4 6
          for num_copies_1 in 4; do # 0 2 4
            num_copies=(${num_copies_1} ${num_copies_2} ${num_copies_3})
            thresh=(${thresh_pt} 0.0 0.0)
            feat_unsup_subset_dir=${data_fmllr_dir}/${TEST_LANG}/"unsup_${nutts_subset}"
            if [ "$nutts_subset" -lt "$nutts" ]; then
              invalid=1
              # If feat dir already exists, check if it is still valid
              [ -d ${feat_unsup_subset_dir} ] && utils/validate_data_dir.sh --no-text ${feat_unsup_subset_dir} && invalid=$?
              # If invalid, create the subset dir
              [ $invalid != 0 ] && utils/subset_data_dir.sh ${feat_unsup_dir} ${nutts_subset} ${feat_unsup_subset_dir}
            fi
            
            nhl2=0
            for nhl1 in 0; do
              for nhl3 in 0; do
                for alpha_3 in 0.001 0.005; do
                  for alpha_2 in 1.6 1.8; do
                    for alpha_1 in 1.0 2.0; do                      
                      etag=type"ssu"_fw${thresh[0]}${thresh[1]}${thresh[2]}_cop${num_copies[0]}${num_copies[1]}${num_copies[2]}_unsup${nutts_subset}_alpha${alpha_1}${alpha_2}${alpha_3}_nhl${nhl1}${nhl2}${nhl3}
                      nnet_outdir=${dnn_dir}/multisoftmax_pt_${etag}
                      mkdir -p $nnet_outdir
                      [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
                      (./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
                        --objective-csl "xent:xent:mse" --lang-weight-csl "${alpha_1}:${alpha_2}:${alpha_3}" --data-type-csl "pt:dt:unsup" --label-type-csl "s:s:f"  \
                        --renew-nnet-type "parallel" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
                        --renew-nnet-opts "--nnet-proto-opts -:-:--no-softmax" --parallel-nhl-opts "$nhl1:-:$nhl3" \
                        --threshold-csl "${thresh[0]}:${thresh[1]}:${thresh[2]}" \
                        --lat-dir-csl "${lats_pt_dir}:-:-" \
                        --dup-and-merge-csl "${num_copies[0]}>>1:${num_copies[1]}>>2:${num_copies[2]}>>3" \
                        ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
                        --min-iters ${min_iters} \
                        ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
                        "${TEST_LANG}:${UNILANG_CODE}:${TEST_LANG}" "${ali_pt_dir}:${ali_dt_dir}:${ali_pt_dir}" \
                        "${feat_pt_dir}:${feat_dt_dir}:${feat_unsup_subset_dir}" \
                        ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
                        ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1;
                        rm -rf ${nnet_outdir}/ali-post/post_combined.ark
                        rm -rf ${nnet_outdir}/ali-post/local/${TEST_LANG}_unsup*/post_train_thresh_*/post.*.ark ) &
                      i=$((i%N_BG)); ((i++==0)) && wait
                    done # alpha_1
                  done # alpha_2
                done # alpha_3
              done # nhl3
            done # nhl1
          done # num_copies_1
        done # num_copies_2
      done # num_copies_3
    done # thresh_pt
  done # nutts_subset
fi
# =========================================

# =========================================
## Train MTL with crowd PT senones: DT phones: Autoencoder; Task 1: CE, Task 2: CE, Task 3: MSE
nutts=6000
unsup_dir_name="unsup_$nutts"
feat_unsup_dir=${data_fmllr_dir}/${TEST_LANG}/${unsup_dir_name}
if [[ $stage -le 61 ]]; then

  # Extract the fMLLR features of unsup data using the PT adapted HMM model
  if [ ! -f ${ali_pt_dir}/decode_${unsup_dir_name}_${TEST_LANG}/lat.1.gz ]; then
    local/get_unsup_lats.sh --nutts ${nutts} --unsup-dir-name ${unsup_dir_name} --skip-decode true \
      ${TEST_LANG} \
      ${ali_pt_dir} \
      ${feat_unsup_dir} \
      ${hmm_dir}/graph_text_G_${TEST_LANG}  \
      dummy  \
      dummy || exit 1;
  fi
  
  feat_pt_dir=${data_fmllr_dir}/${TEST_LANG}/train
  feat_dt_dir=${data_fmllr_dir}/${UNILANG_CODE}/train
  
  i=1
  for nutts_subset in 4000; do  # 4000 3000 2000 1000
    for thresh_pt in 0.6 ; do  # 0.6 0.7 0.8
      for num_copies_3 in 0; do # 0 1
        for num_copies_2 in 0 2; do # 0 2 4 6
          for num_copies_1 in 4; do # 0 2 4
            num_copies=(${num_copies_1} ${num_copies_2} ${num_copies_3})
            thresh=(${thresh_pt} 0.0 0.0)
            feat_unsup_subset_dir=${data_fmllr_dir}/${TEST_LANG}/"unsup_${nutts_subset}"
            if [ "$nutts_subset" -lt "$nutts" ]; then
              invalid=1
              # If feat dir already exists, check if it is still valid
              [ -d ${feat_unsup_subset_dir} ] && utils/validate_data_dir.sh --no-text ${feat_unsup_subset_dir} && invalid=$?
              # If invalid, create the subset dir
              [ $invalid != 0 ] && utils/subset_data_dir.sh ${feat_unsup_dir} ${nutts_subset} ${feat_unsup_subset_dir}
            fi
            
            nhl2=0
            for nhl1 in 0; do
              for nhl3 in 0; do
                for alpha_3 in 0.001 0.005; do
                  for alpha_2 in 1.6 1.8 2.0; do
                    for alpha_1 in 1.0 2.0; do
                      etag=type"spu"_fw${thresh[0]}${thresh[1]}${thresh[2]}_cop${num_copies[0]}${num_copies[1]}${num_copies[2]}_unsup${nutts_subset}_alpha${alpha_1}${alpha_2}${alpha_3}_nhl${nhl1}${nhl2}${nhl3}
                      nnet_outdir=${dnn_dir}/multisoftmax_pt_${etag}
                      mkdir -p $nnet_outdir
                      [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
                      (./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
                        --objective-csl "xent:xent:mse" --lang-weight-csl "${alpha_1}:${alpha_2}:${alpha_3}" --data-type-csl "pt:dt:unsup" --label-type-csl "s:p:f"  \
                        --renew-nnet-type "parallel" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
                        --renew-nnet-opts "--nnet-proto-opts -:-:--no-softmax" --parallel-nhl-opts "$nhl1:-:$nhl3" \
                        --threshold-csl "${thresh[0]}:${thresh[1]}:${thresh[2]}" \
                        --lat-dir-csl "${lats_pt_dir}:-:-" \
                        --dup-and-merge-csl "${num_copies[0]}>>1:${num_copies[1]}>>2:${num_copies[2]}>>3" \
                        ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
                        --min-iters ${min_iters} \
                        ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
                        "${TEST_LANG}:${UNILANG_CODE}:${TEST_LANG}" "${ali_pt_dir}:${ali_mono_dt_dir}:${ali_pt_dir}" \
                        "${feat_pt_dir}:${feat_dt_dir}:${feat_unsup_subset_dir}" \
                        ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
                        ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1;
                        rm -rf ${nnet_outdir}/ali-post/post_combined.ark
                        rm -rf ${nnet_outdir}/ali-post/local/${TEST_LANG}_unsup*/post_train_thresh_*/post.*.ark ) &
                      i=$((i%N_BG)); ((i++==0)) && wait
                    done # alpha_1
                  done # alpha_2
                done # alpha_3
              done # nhl3
            done # nhl1
          done # num_copies_1
        done # num_copies_2
      done # num_copies_3
    done # thresh_pt
  done # nutts_subset
fi
# =========================================

# =========================================
## MTL trained with crowd PT senones, DT senones, and ASR ST senones (self-training); Task 1: CE, Task 2: CE, Task 3: CE
nutts=4000
thresh=0.6
num_copies=0
unsup_dir_name="unsup_$nutts"
feat_unsup_dir=${data_fmllr_dir}/${TEST_LANG}/${unsup_dir_name}
decoding_mdl_dir=${dnn_dir}/multisoftmax_pt_fw${thresh}_cop${num_copies}/decode_block_1_dev_text_G_${TEST_LANG} # dnn mdl directory used to decode the unsup data
lats_unsup_dir=${dnn_dir}/multisoftmax_pt_fw${thresh}_cop${num_copies}/decode_${unsup_dir_name}_text_G_${TEST_LANG} # dir where lattices generated by decoding unsup data will be saved
if [[ $stage -le 70 ]] && false; then

# Now decode the unsupervised data using a reasonably well trained DNN model
local/get_unsup_lats.sh --nutts ${nutts} --unsup-dir-name ${unsup_dir_name} ${TEST_LANG} ${ali_pt_dir} ${feat_unsup_dir} \
  ${hmm_dir}/graph_text_G_${TEST_LANG}  ${decoding_mdl_dir}  ${lats_unsup_dir} || exit 1;

feat_pt_dir=${data_fmllr_dir}/${TEST_LANG}/train
feat_dt_dir=${data_fmllr_dir}/${UNILANG_CODE}/train

i=1
for nutts_small_unsup in 4000 ; do  # 4000 3000 2000 1000
  for thresh_pt in 0.6 ; do  # 0.6 0.7 0.8
    for thresh_unsup in 0.9; do # 0.7 0.9

      for num_copies_3 in 0 1 2 3 4; do # 0 1
        for num_copies_2 in 0; do # 0 2 4 6
          for num_copies_1 in 0 2 4; do # 0 2 4
          
            num_copies=(${num_copies_1} ${num_copies_2} ${num_copies_3})
            thresh=(${thresh_pt} 0.0 ${thresh_unsup})
            nutts_small=${nutts_small_unsup}
           
            unsupsmall_dir_tag="unsup_${nutts_small}" 
            feat_unsupsmall_dir=${data_fmllr_dir}/${TEST_LANG}/${unsupsmall_dir_tag}
           
            if [ "$nutts_small" -lt "$nutts" ]; then
              utils/subset_data_dir.sh ${feat_unsup_dir} ${nutts_small} ${feat_unsupsmall_dir}
            fi
   
            etag=type"sss"_fw${thresh[0]}${thresh[1]}${thresh[2]}_cop${num_copies[0]}${num_copies[1]}${num_copies[2]}_unsup${nutts_small}
            nnet_outdir=${dnn_dir}/multisoftmax_pt_$etag
            mkdir -p $nnet_outdir
            [ -f ${nnet_outdir}/final.nnet ] && echo "${nnet_outdir}/final.nnet exists. Skipping this run" && continue
            ( ./run_dnn_multilingual.sh --dnn-init "${dnn_dir}/monosoftmax_dt/final.nnet" \
              --objective-csl "xent:xent:xent" --lang-weight-csl "1.0:1.0:1.0" --data-type-csl "pt:dt:semisup"  --label-type-csl "s:s:s" \
              --renew-nnet-type "blocksoftmax" --randomizer-size ${randomizer_size} --minibatch-size ${minibatch_size} \
              --threshold-csl "${thresh[0]}:${thresh[1]}:${thresh[2]}" \
              --lat-dir-csl "${lats_pt_dir}:-:${lats_unsup_dir}" \
              --dup-and-merge-csl "${num_copies[0]}>>1:${num_copies[1]}>>2:${num_copies[2]}>>1" \
              ${cmvn_opts:+ --cmvn-opts "$cmvn_opts"} --delta-order $delta_order --splice $splice --splice-step $splice_step \
              --min-iters ${min_iters} \
              ${parallel_opts:+ --parallel-opts "$parallel_opts"} \
              "${TEST_LANG}:${UNILANG_CODE}:${TEST_LANG}" "${ali_pt_dir}:${ali_dt_dir}:${ali_pt_dir}" \
              "${feat_pt_dir}:${feat_dt_dir}:${feat_unsupsmall_dir}" \
              ${data_fmllr_dir}/${TEST_LANG}/combined_$etag \
              ${nnet_outdir} 2>&1 | tee ${nnet_outdir}/run_dnn_multilingual.log || exit 1; ) &
            i=$((i%N_BG)); ((i++==0)) && wait
          done # num_copies_1          
        done # num_copies_2
      done # num_copies_3
      
    done # thresh_unsup
  done # thresh_pt
done # nutts_small_unsup

fi
# =========================================
